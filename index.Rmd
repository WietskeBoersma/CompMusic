---
title: "Computational Musicology Storyboard 30-03-2025"
output: flexdashboard::flex_dashboard
---

```{r setup, include=FALSE}

library(tidyverse) 
library(flexdashboard)
library(gridExtra) 
library(rjson) 
library(ggplot2)
source("compmus.R")
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction {data-icon="fa-headphones"}

Structure
-----------------------------------------------------------------------

#### Welcome to my portfolio for Computational Musicology!

For this portfolio I analyzed two songs using a variety of different metrics. These metrics helped me understand how my songs were structured, including aspects such as timbre and tempo. By applying computational methods, I was able to extract meaningful insights and identify patterns within the songs. The analysis provided a deeper understanding of how musical elements come together to form the unique sound of each track.

For this project I used an AI-tool to generate the tunes, namely JenAI, the idea of these two prompts for the tunes were specified by using ChatGPT (I made two prompts myself and let ChatGPT generate even more specific prompts)

#### Song #1

For 'wietske-b-1.mp3' I used: Create a modern indie-pop track with a warm, intimate vibe, blending organic acoustic elements with subtle electronic textures. The song should feature delicate yet expressive string arrangements (such as a small string ensemble or chamber-style strings) that add depth and emotion without overpowering the core melody. The instrumentation should include gentle guitar or piano, soft percussion, and atmospheric pads or synths to enhance the dreamy, introspective feel. The track should be between 2 to 4 minutes long, suitable for a public broadcaster.

And this is what it sounds like:

```{r, echo=FALSE, results="asis"}
library(jsonlite)

# Load JSON
json_data <- fromJSON("features/wietske-b-1.json")

# Extract the MP3 file name
audio_file <- json_data$metadata$tags$file_name

# Construct path (since Shiny serves files from 'www/')
audio_path <- paste0("www/", audio_file)

# Output the HTML audio player
cat(paste0('<audio controls preload="none">
              <source src="', audio_path, '" type="audio/mpeg">
              Your browser does not support the audio element.
           </audio>'))
```

#### Song #2

For 'wietske-b-2.mp3' I used: Create a high-energy pop-rock track infused with modern synth elements. The song should feature driving drums, a tight bassline, and rhythmic electric guitars with a mix of clean and overdriven tones. Synths should add depth with lush pads, arpeggiated sequences, and subtle electronic effects. The track should feel anthemic and uplifting, with a dynamic build and a powerful, memorable chorus. 2-4 minutes.

And this is what it sounds like:

```{r, echo=FALSE, results="asis"}
library(jsonlite)

# Load JSON
json_data <- fromJSON("features/wietske-b-2.json")

# Extract the MP3 file name
audio_file <- json_data$metadata$tags$file_name

# Construct path (since Shiny serves files from 'www/')
audio_path <- paste0("www/", audio_file)

# Output the HTML audio player
cat(paste0('<audio controls preload="none">
              <source src="', audio_path, '" type="audio/mpeg">
              Your browser does not support the audio element.
           </audio>'))

```

#### Structure

For this portfolio I will walk you through the analysis of these two songs. We will first focus on the analysis of just the two songs. From timbre, to harmony and then to rhythm. This reflects how we perceive music - first the overall sound color, then the harmonic structure, and finally the rhythm. And finally, compare the songs to the entire class corpus to analyze how they are positioned within the dataset in terms of musical features.


# Timbre {data-icon="fa-palette"}

Visual Song #1
-----------------------------------------------------------------------

### Cepstrogram Song #1
```{r,echo=FALSE}
cepto1 <- "features/wietske-b-1.json" |>                           # Change the track
  compmus_mfccs(norm = "euclidean") |>                  # Change the norm
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL
  ) +
    scale_fill_viridis_c(option = "plasma", name = "") + # Added name for the legend
  labs(x = "Time (s)", y = "Coefficient Number") + # Removed fill = NULL
  theme_classic()                                       # Change the theme?

print(cepto1)
```

### Timbre-based SSM Song #1
```{r, echo=FALSE}
selft1 <- "features/wietske-b-1.json" |>                           # Change the track
  compmus_mfccs(norm = "euclidean") |>                  # Change the norm
  compmus_self_similarity(
    feature = mfcc,
    distance = "euclidean"                             # Change the distance
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(option = "inferno", name = "Similarity") +               # Change the colours?
  labs(x = "Time (s)", y = "Time (s)") +
  theme_classic()                                      # Change the theme?

print(selft1)
```

Visuals Song #2
-----------------------------------------------------------------------

### Cepstrogram Song #2

```{r, echo=FALSE}
cepto2 <- "features/wietske-b-2.json" |>                           # Change the track
  compmus_mfccs(norm = "euclidean") |>                  # Change the norm
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL
  ) +
  scale_fill_viridis_c(option = "plasma", name = "") + # Added name for the legend
  labs(x = "Time (s)", y = "Coefficient Number") + # Removed fill = NULL
  theme_classic() 

print(cepto2)
```

### Timbre-based SSM Song #2
```{r, echo=FALSE}
selft2 <- "features/wietske-b-2.json" |>                           # Change the track
  compmus_mfccs(norm = "euclidean") |>                  # Change the norm
  compmus_self_similarity(
    feature = mfcc,
    distance = "euclidean"                             # Change the distance
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(option = "inferno", name = "Similarity") +               # Change the colours?
  labs(x = "Time (s)", y = "Time (s)") +
  theme_classic()                                      # Change the theme?

print(selft2)

```

Description
-----------------------------------------------------------------------

### Cepstrograms 

The cepstrograms help us see how the 'sound color' of a song changes. Dark blue means a quiet sound feature, and yellow means a loud one. The bottom of the diagram shows the basic tones, and the top shows the finer details like brightness and texture.

In the first song's diagram, the colors stay mostly yellow and orange, meaning the sound color stays pretty steady. There are no big jumps from dark to bright, so the song doesn't have any sudden loud parts or dramatic changes. The bottom of the picture is also bright, showing that the main tones are strong. The top is mostly the same color, meaning the sound texture is stable. This matches what we hear: the instruments play steadily, and the sound color doesn't change much.

The second song's diagram is a bit different. We see a little more activity at the top, around number 3, but the strongest activity is still around number 1. This means the song has some small sound changes, but nothing too big. It also suggests we're hearing two main instruments with different sound colors. When we listen, we can clearly hear the electric guitar and drums, which could correspond to the activity around the two coefficients. These instruments create the main sound and feel of this song.

### SSM

Looking at the self-similarity matrix (SSM) for the first song, we can see clear 'block-like' patterns. These patterns tell us that there are sections of the song where the overall sound, or timbre, stays very consistent. For example, there's a large block that starts around 60 seconds and lasts until about 120 seconds. If we listen to the song, we can hear a change around 50-60 seconds, and then the sound remains quite homogenous for the next minute. We also notice a similar block from about 130 to 170 seconds.

Interestingly, apart from the main diagonal line (which just shows that every moment is identical to itself), there are no clear diagonal lines. This suggests that while the song might feel repetitive, it doesn't have repeating patterns of timbre. Instead, it has sections where the overall 'sound color' remains the same. The predominantly dark tones in the SSM indicate that, on a timbre level, the song is fairly consistent throughout.

For the second song the SSM is a bit harder to analyze, as there is no classic block-like structure nor a classic path-like structure present in the diagram. There seems to be a 'build-up' in similarity concerning the timbre in the song, at the start there seems to be a lot more dissimilar parts, then in the second part. In the second part of the song there seems to be a block starting at around 75 seconds until the end, which contains very similar parts concerning the timbre of the song, with only one 'stripe' of dissimilarity crossing it vertically (and horizontally of course). 

### Conclusion

Concluding, there are quite a few differences between the two songs, if you're looking at the timbre of the songs. While the timbre of the first song seems to stay stable at coefficient = 1, the timbre of the second song seems to include two distinct sound colors, probably caused by the two main instruments you hear; electric guitar and drums. While the timbre of the first song seems to be more homogenous at certain time stamps in the song, the second song seems to become more and more similar while the song progresses.

# Harmony & Pitch {data-icon="fa-music"}

Chromagrams
-----------------------------------------------------------------------

### Chromagram Song #1

```{r, echo=FALSE}
chroma1 <- "features/wietske-b-1.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(name="",option = "plasma") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()   # Change the theme?

print(chroma1)
```

### Chromagram Song #2
```{r, echo=FALSE}
chroma2 <- "features/wietske-b-2.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(name="", option = "plasma") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()   # Change the theme?

print(chroma2)
```


Chordograms
-----------------------------------------------------------------------

### Song #1 Chordogram
```{r, echo=FALSE}
library("tibble")

# C C# D Eb E F F# G Ab A Bb B

major_chord <- 
  c( 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0) 
minor_chord <-
  c( 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0) 
seventh_chord <- 
  c( 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0)

major_key <- 
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88) 
minor_key <- 
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <- 
  tribble( 
    ~name, ~template, 
    "Gb:7", circshift(seventh_chord, 6), 
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10), 
    "Db:maj", circshift(major_chord, 1), 
    "F:min", circshift(minor_chord, 5), 
    "Ab:7", circshift(seventh_chord, 8), 
    "Ab:maj", circshift(major_chord, 8), 
    "C:min", circshift(minor_chord, 0), 
    "Eb:7", circshift(seventh_chord, 3), 
    "Eb:maj", circshift(major_chord, 3), 
    "G:min", circshift(minor_chord, 7), 
    "Bb:7", circshift(seventh_chord, 10), 
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2), 
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5), 
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0), 
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4), 
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7), 
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2), 
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6), 
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9), 
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4), 
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8), 
    "B:7", circshift(seventh_chord, 11), 
    "B:maj", circshift(major_chord, 11), 
    "D#:min",circshift(minor_chord, 3) 
  )

key_templates <- tribble( ~name, ~template, "Gb:maj",
circshift(major_key, 6), "Bb:min", circshift(minor_key, 10), "Db:maj",
circshift(major_key, 1), "F:min", circshift(minor_key, 5), "Ab:maj",
circshift(major_key, 8), "C:min", circshift(minor_key, 0), "Eb:maj",
circshift(major_key, 3), "G:min", circshift(minor_key, 7), "Bb:maj",
circshift(major_key, 10), "D:min", circshift(minor_key, 2), "F:maj",
circshift(major_key, 5), "A:min", circshift(minor_key, 9), "C:maj",
circshift(major_key, 0), "E:min", circshift(minor_key, 4), "G:maj",
circshift(major_key, 7), "B:min", circshift(minor_key, 11), "D:maj",
circshift(major_key, 2), "F#:min", circshift(minor_key, 6), "A:maj",
circshift(major_key, 9), "C#:min", circshift(minor_key, 1), "E:maj",
circshift(major_key, 4), "G#:min", circshift(minor_key, 8), "B:maj",
circshift(major_key, 11), "D#:min", circshift(minor_key, 3) )

chord1 <- "features/wietske-b-1.json" |> 
compmus_chroma(norm = "manhattan") |>
compmus_match_pitch_templates(
  chord_templates, 
  norm = "identity",
  distance = "cosine"
)|> 

ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() + 
  scale_fill_viridis_c(name = "") + 
  labs(x = "Time (s)", y = "", fill = NULL) + 
  theme_classic() 
# Try different norms (and match it with what you used in `compmus_chroma`)
# Try different distance metrics ) 

print(chord1)
```

### Song #2 Chordogram
```{r, echo=FALSE}
chord2 <- "features/wietske-b-2.json" |> 
compmus_chroma(norm = "manhattan") |>
compmus_match_pitch_templates(
  chord_templates, 
  norm = "identity",
  distance = "cosine"
)|> 

ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() + 
  scale_fill_viridis_c(name = "") + 
  labs(x = "Time (s)", y = "", fill = NULL) + 
  theme_classic() 

print(chord2)
# Try different norms (and match it with what you used in `compmus_chroma`)
# Try different distance metrics ) 
```

Description
-----------------------------------------------------------------------
So now we're moving on to the harmony of the songs. The harmony of these two songs are shown using the chromagrams and chordograms. When comparing the chromagrams you can spot a few differences. For the first song you can see the pitch classes around F# / Gb are mostly light colored, which indicates a presence of these classes. In the last part the lower pitch classes like A and B become more present. Overall there is a lot of variation in pitch classes within this song. This also counts for the second song, where the start is mostly dominated by the lowest pitch class B, but this presence declines, a bit and other pitch classes become more present, such as C, D and F#/Gb. 
The presence of the lower pitch class in the second song can be derived from the fact that mostly electric guitar and drums are present, which produce lower pitches like B.

The chordograms are not as intuitive as the chromagrams, it is harder to gain insight in what is actually shown. For both I used the cosine distance and the chord templates. The lighter parts are the chords played, as you can see for both songs it seems there are a lot of different chords played at the same time, which is very confusing. In this case it is therefore better to focus on the chromagrams, when analyzing the harmony of both songs.


# Harmony & Pitch 2 {data-icon="fa-music"}

Chroma-based Self Similarity Matrices
-----------------------------------------------------------------------

### Song #1 Chroma-based Self Similarity Matrix

```{r, echo=FALSE}

selfc1 <- "features/wietske-b-1.json" |>                           # Change the track
  compmus_chroma(norm = "euclidean") |>                 # Change the norm
  compmus_self_similarity(
    feature = pc,
    distance = "euclidean"                             # Change the distance
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(option = "plasma", guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()                                      # Change the theme?
  coord_fixed(ratio = 1)
  
  print(selfc1)

```

### Song #2 Chroma-based Self Similarity Matrix
```{r, echo=FALSE}

selfc2 <- "features/wietske-b-2.json" |>                           # Change the track
  compmus_chroma(norm = "euclidean") |>                 # Change the norm
  compmus_self_similarity(
    feature = pc,
    distance = "euclidean"                             # Change the distance
  ) |>   

  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(option = "plasma", guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()                                      # Change the theme?
  coord_fixed(ratio = 1)

  print(selfc2)
```

Description
-----------------------------------------------------------------------

Just like we looked at the timbre-based SSMs, we can also look at SSMs based on chroma, which tells us about the chords and pitches in the songs. In both songs, we see 'block-like' patterns, meaning there are sections where the chords and pitches stay pretty much the same. However, the size of these blocks is different.

In the first song, the blocks are quite long, about 10-20 seconds. This means that for those periods, the chords and pitches are very homogenous But in the second song, the blocks are shorter, usually no more than 5-10 seconds. This tells us that the chords and pitches change more often in the second song.

So, we can say that the first song has longer stretches where the chords and pitches are stable, while the second song has more frequent changes. This fits with what we saw in the chromagrams, where the first song also showed more stable chroma than the second song.

NB: I am aware that there is code behind the diagrams, but I can't figure out how to get rid of it.

# Novelty functions {data-icon="fa-bolt"}

Visual Song #1
-----------------------------------------------------------------------

### Energy-based Song #1

```{r, echo=FALSE}
nove1 <- "features/wietske-b-1.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Energy Novelty")

print(nove1)
```

### Spectral-Based Song #1
```{r, echo=FALSE}
novs1 <- "features/wietske-b-1.json" |>
  compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Spectral Novelty")

print(novs1)
```

Visual Song #2
-----------------------------------------------------------------------

### Energy-based Song #2

```{r, echo=FALSE}
nove2 <- "features/wietske-b-2.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Energy Novelty")

print(nove2)
```

### Spectral-based Song #2
```{r, echo=FALSE}
novs2 <- "features/wietske-b-2.json" |>
  compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Spectral Novelty")

print(novs2)
```

Description
-----------------------------------------------------------------------

We have used two different types of novelty functions. Namely, energy-based novelty functions and spectral-based novelty functions. The energy-based novelty function is better suited for songs with a strong onset/beat. Spectral-based novelty is more suitable for songs with a more melodic sound, and a less present beat. If we look at the first energy-based function it shows a few peaks, but not as much as it does with the second song, where there seems to be a bit more of a stable beat. The spectral-based function shows a more interesting diagram for the first song, compared to the energy-based function. This can be explained by the fact that this song is more melodic, and does not have such a strong beat, which makes the spectral-based function more fitting and interesting. On the other hand, for the second song is the energy-based function a better fit. The drums cause a strong beat which is more fitting for the energy-based function. For example, it tells us there is a big onset around 150 seconds, which is clearly audible in the song.


# Rhythm and Tempo {data-icon="fa-drum"}

Visuals Song cyclic is True
-----------------------------------------------------------------------

### Tempogram Song #1
```{r, echo=FALSE}
tempo1t <- "features/wietske-b-1.json" |> compmus_tempogram(window_size
= 8, hop_size = 1, cyclic = TRUE) |> ggplot(aes(x = time, y = bpm,
fill = power)) + geom_raster() + scale_fill_viridis_c(guide = "none") +
labs(x = "Time (s)", y = "Tempo (BPM)") + theme_classic()

print(tempo1t)
```

### Tempogram Song #2

```{r, echo=FALSE}
tempo2t <- "features/wietske-b-2.json" |> compmus_tempogram(window_size
= 8, hop_size = 1, cyclic = TRUE) |> ggplot(aes(x = time, y = bpm,
fill = power)) + geom_raster() + scale_fill_viridis_c(guide = "none") +
labs(x = "Time (s)", y = "Tempo (BPM)") + theme_classic()

print(tempo2t)
```

Description
-----------------------------------------------------------------------

Now onto the tempo and rhythm of my songs. These are the tempograms for the two songs. It shows an alternating tempo of around 140 BPM. Although the song does sound a bit repetitive, and this was also supported by both SSM's, the tempogram does not really show this. This alternating tempo is probably caused by the fact that it is, as metioned before, more of a melodic song than a song with a strict tempo. 

On the other hand, the second song does seem to have quite a strict tempo, which is a bit higher than the other one, it's about 150 BPM. This can be derived, again, from the drums which produce a strong beat.

Therefore we can conclude that a tempogram might not be as informative nor relevant for the first song as it is for the second song.


```{r, echo=FALSE}
library(tidymodels)
library(ggdendro)
library(heatmaply)

source("compmus.R")

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

compmus2025 <- read_csv("compmus2025.csv")

cluster_juice <-
  recipe(
    filename ~
      arousal +
      danceability +
      instrumentalness +
      tempo +
      valence,
    data = compmus2025
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(compmus2025) |>
  juice() |>
  column_to_rownames("filename")

compmus_dist <- dist(cluster_juice, method = "euclidean")
```

# Class Corpus AI Analysis: Heat Map {data-icon="fa-chart-area"}

HeatMap
-----------------------------------------------------------------------

```{r}
heatmaply(
  cluster_juice,
  hclustfun = hclust,
  hclust_method = "average",  # Change for single, average, or complete linkage.
  dist_method = "euclidean"
)
```

Description
-----------------------------------------------------------------------
Now that we have analyzed both songs deeply, we are moving on to the last part of this portfolio: the class corpus. We are gonna take a look at how my songs fit into the class corpus. First up: The Heat Map. This heat map gives insight in where my songs are placed compared to the other songs in the corpus concerning the valence, arousal, danceability, instrumentalness and tempo. To look at the heat map properly, you need to zoom in a bit.

For the first song, the valence and arousal show quite low values around -0.8. While the danceability (0.4), instrumentalness (0.3) and tempo (1.3) show increasingly higher values.

For the second song, it shows a valence, instrumentalness and arousal value of around 0. Danceability of around 1.6, and tempo shows a relatively low value of around -1.1.

From this we can conclude that the first song has high valence and arousal, while the second song has a medium valence and arousal. The instrumentalness of the first song is medium, just like the second song. The danceability for the first song is also medium, and low for the second song. And the tempo is relatively low for the first song and high for the second (as we already knew).

# Class Corpus AI Analysis: Classification {data-icon="fa-chart-area"}

```{r}
compmus2025_filtered <- 
  compmus2025 |> filter(!is.na(ai)) |> 
  mutate(ai = factor(if_else(ai, "AI", "Non-AI")))

classification_recipe <-
  recipe(
    ai ~
      arousal +
      danceability +
      instrumentalness +
      tempo +
      valence,
    data = compmus2025_filtered
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

compmus_cv <- compmus2025_filtered |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")

classification_knn <- 
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(compmus_cv, control = control_resamples(save_pred = TRUE))

classification_knn |> get_conf_mat()

classification_knn |> get_conf_mat() |> autoplot(type = "mosaic")

classification_knn |> get_conf_mat() |> autoplot(type = "heatmap")

classification_knn |> get_pr()


```

Description
-----------------------------------------------------------------------

We also built a classification model to using the class corpus, namely a KNN-model. It is classifier that predicts whether a song in the class corpus is AI-generated or not. Looking at the precision and recall of the model we can conclude that the AI class has higher precision and recall than the Non-AI class, suggesting that the model is better at predicting AI-generated songs. However, there’s still room for improvement in both precision and recall for both classes, since none of the values are particularly high.


# Class Corpus AI Analysis: Random Forest {data-icon="fa-chart-area"}

```{r}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")

indie_forest <- 
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    compmus_cv, 
    control = control_resamples(save_pred = TRUE)
  )

indie_forest |> get_pr()

workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(forest_model) |> 
  fit(compmus2025_filtered) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")

compmus2025_filtered |>
  ggplot(aes(x = valence, y = arousal, colour = ai, size = tempo)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Valence",
    y = "Arousal",
    size = "Tempo",
    colour = "AI"
  )


```

Description
-----------------------------------------------------------------------

On to the last part; the Random Forest. First we created a feature importance diagram, where arousal was apparently the most important feature. We used this for the Random Forest and the scatter plot shows the result. From the scatter plot you can't really draw any straight forward conclusions, as the yellow and purple dots don't seem to cluster together as clearly as we would have liked to see. Bigger dots represent higher tempo, and even this feature can't be used to draw any hard conclusions. Therefore this random forest does not bring us any new information, like we hoped it would.

# Conclusion

During this project I analyzed two of my own AI-generated songs. These songs where two very different songs, and this also resulted from the analyses I performed. 

From the Self-Similarity Matrices to the Tempograms, almost every diagram showed the difference between these two, in all kinds of ways. It showed that the first song has more of a melodic tune with more homogenous sound of longer duration, aswell for timbre as for chroma. The melodic tune of the song is also supported by the spectral-based novelty function, which was a much better fit than the energy-based function. Also the pitch seemed to be a bit higher compared to the second song, which can be derived from the chromagrams.

For the second song it showed that there was a strong beat, produced by the drums most likely, in the tempogram and in the energy-based novelty function, which came out as a better fit for this song. The pitch seemed to be a bit lower overall compared to the first song, following the chromagram. Also the presence of two main instruments, namely the drums and the electric guitar made the cepstrogram stand out from the one from the first song.

Overall this project helped me gain an insight in the differences between songs, and on how many levels this can be analyzed and justified, with the support of all these different types of diagrams. It is truly mindblowing!
